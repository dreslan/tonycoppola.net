<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Superintelligence on Tony Coppola</title>
    <link>https://tonycoppola.net/tags/superintelligence/</link>
    <description>Recent content in Superintelligence on Tony Coppola</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 May 2025 11:31:51 -0400</lastBuildDate>
    <atom:link href="https://tonycoppola.net/tags/superintelligence/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Alignment Challenge</title>
      <link>https://tonycoppola.net/quotes/yudkowsky-ai-alignment/</link>
      <pubDate>Wed, 05 Jul 2023 12:00:00 -0400</pubDate>
      <guid>https://tonycoppola.net/quotes/yudkowsky-ai-alignment/</guid>
      <description>&lt;p&gt;The concern is not that AI systems might spontaneously decide to hurt us. The concern is that we might build AI systems that are caught in adversarial dynamics with respect to their users, with us, with society, with other AI systems, or that magnify the ability of some humans to hurt other humans. The concern is that there are possible AI designs that let them be much more powerful than humans, and that we might deploy systems with those designs too quickly.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
